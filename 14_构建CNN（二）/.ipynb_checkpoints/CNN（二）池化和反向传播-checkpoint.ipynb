{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.zero_padding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "padding：\n",
    "    可以让我们既使用卷积层又可以避免矩阵的尺寸越来越小，对于很深的神经网络意义很大，因为如果不使用padding，那么每卷积一次矩阵的尺寸就会变小，等到很深的网络层时，矩阵就没有了\n",
    "    有助于保留图像四周的信息，如果没有padding，图像四周的元素被卷积的次数就很少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    作用：给样本集X的所有样本进行零填充\n",
    "    \n",
    "    参数：\n",
    "    X -- 样本集，维度是（m, n_H, n_W, n_c）\n",
    "    pad -- padding的个数\n",
    "    \n",
    "    返回值：\n",
    "    X_pad -- 返回填补后的样本集。维度是（m, n_H + 2*pad, n_W+ 2*pad, n_c）\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.pad是numpy提供的一个零补函数\n",
    "    X_pad = np.pad(X, ((0, 0),(pad, pad),(pad, pad),(0, 0)),'constant',constant_values = 0)\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape (4, 3, 3, 2)\n",
      "x_pad.shape (4, 7, 7, 2)\n",
      "x[1,1] : [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#单元测试\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3,3,2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print('x.shape',x.shape)\n",
    "print('x_pad.shape',x_pad.shape)\n",
    "print('x[1,1] :',x[1,1])\n",
    "print('x_pad[1,1]',x_pad[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d0f0ad0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.单步卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    作用：只执行一步卷积\n",
    "    \n",
    "    参数：\n",
    "    a_slice_prov -- 输入 矩阵中和过滤器卷积的那一块数据\n",
    "                 -- 这里的输入矩阵也就是上一层的输出矩阵\n",
    "    W -- 权重参数w。这里指的是过滤器，过滤器就是权重参数w\n",
    "      -- 维度是（f, f, n_C_prev）,与a_slice_prev是一样的，因为是它俩进行卷积，所以维度是一样的\n",
    "    b -- 阈值b，教程中我们说过每一个过滤器会有一个对应的阈值 维度是（1，1，1）\n",
    "    \n",
    "    返回值：\n",
    "    Z -- 卷积一步后得到的一个数值，这个数值是一个输出矩阵的一个元素\n",
    "\n",
    "    \"\"\"\n",
    "    #将a_slice_prev与W中的每一个元素进行相乘\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    #将上面相乘的结果累加起来\n",
    "    Z = np.sum(s)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z: -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "#单元测试\n",
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print('Z:',Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个过滤器生成矩阵后构建多维矩阵"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "可以使用多个过滤器。每个过滤器卷积之后都会得到一个二维矩阵。将多个过滤器得到的二维矩阵叠加起来构成多维矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "<center>\n",
    "    <video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "    </video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取子矩阵"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python中，如果想获取矩阵中某一区域的元素，例如一个（5，5，5）矩阵中的左上角的一个2 x 2的子矩阵，那么可以使用如下代码：\n",
    "    a_slice_prev = a_prev[0:2, 0:2, :]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python中，通过开始索引和结束索引，可以获取数组中某一区域的元素，\n",
    "为了从矩阵中获取一个子矩阵，首先需要计算出子矩阵在母矩阵中的坐标，\n",
    "纵向开始索引vert_start,纵向结束索引vert_end\n",
    "横向开始索引horiz_start,横行结束索引horiz_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **图 3** </u><font color='purple'>  : **定位子矩阵** <br> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算输出矩阵的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{这个是过滤器的个数}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    实现卷积网络的前向传播\n",
    "    \n",
    "    参数：\n",
    "    A_prev -- 本层的输入矩阵，也就是上一层的输出矩阵。\n",
    "           -- 维度是（m, n_H_prev, n_W_prev, n_C_prev）\n",
    "    W -- 权重，也就是过滤器。维度是（f, f, n_C_prev, n_C），n_C表示过滤器的个数\n",
    "    b -- 阈值，维度是（1，1，1，n_C）。一个过滤器一个阈值\n",
    "    hparameters -- 数组参数步长stride数s和padding数p\n",
    "    \n",
    "    返回值：\n",
    "    Z -- 输出矩阵，也就是卷积结果。维度是（m, n_H, n_W, n_C）\n",
    "    cache -- 缓存一些数值， 以供反向传播时用\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "    stride = hparameters['stride'] # 步长s\n",
    "    pad = hparameters['pad'] # 填补数量p\n",
    "    \n",
    "    # 计算输出矩阵的维度。参考上面提供的公式    \n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1 # 使用int()来实现向下取整\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # 初始化输出矩阵\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # 给输入矩阵进行padding填补0\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                                 # 遍历每一个样本\n",
    "        a_prev_pad = A_prev_pad[i]                     # 取出一个样本对应的输入矩阵\n",
    "        for h in range(n_H):                           # 遍历输出矩阵的高\n",
    "            for w in range(n_W):                       # 遍历输出矩阵的宽\n",
    "                for c in range(n_C):                   # 遍历每一个过滤器\n",
    "                    # 计算出输入矩阵中本次应该卷积的区域的索引，然后通过这些索引取出将被卷积的小块数据。\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # 利用之前我们实现的conv_single_step函数来对这块数据进行卷积。\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                                        \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    " \n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =  0.15585932488906465\n",
      "cache_conv[0][1][2][3] =  [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "#单元测试\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {'pad' : 2,\n",
    "               'stride' : 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean = \",np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] = \",cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化层"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "池化层可以将矩阵的尺寸变小，池化层不仅仅可以降低神经网络的计算量，还可以让网络的预测鲁棒性更好\n",
    "两种池化：\n",
    "* 最大池化层\n",
    "* 平均池化层\n",
    "池化层是没有参数的，因为它的过滤器是虚拟的是不存在的，但它有超参数，例如窗口大小f，也就是指定池化的子矩阵的高宽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化的前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面实现一个池化的前向传播，可以指定用最大池化还是平均池化。\n",
    "计算输出矩阵的维度公式会与卷积层的有所不同，因为没有padding：\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev,hparameters, mode = 'max'):\n",
    "    \"\"\"\n",
    "    实现池化的前向传播\n",
    "    \n",
    "    参数：\n",
    "    A_prev\n",
    "    hparameters\n",
    "    mode -- 池化模式，最大池化就写max，平均池化就写average\n",
    "    \n",
    "    返回值：\n",
    "    A -- 池化层的输出矩阵，维度（m, n_H, n_W, n_C）\n",
    "    cache\n",
    "    \"\"\"\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    #窗口大小\n",
    "    f = hparameters['f']\n",
    "    stride = hparameters['stride']\n",
    "    \n",
    "    #计算输出矩阵的大小\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C =n_C_prev\n",
    "    \n",
    "    #初始化输出矩阵\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    #根据mode，执行池化\n",
    "                    if mode == 'max':\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == 'average':\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "#单元测试\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {'stride' : 1, 'f' : 4}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print('mode = max')\n",
    "print('A =', A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = 'average')\n",
    "print('mode = average')\n",
    "print('A =', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以上完成了CNN的前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN的反向传播\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在使用框架后只需要实现前向传播就可以了，框架会自动帮助我们实现反向传播，\n",
    "在之前的升级网络反向传播中，通过计算成本函数相关偏导数来不断更新参数w和b的值，其实在CNN的反向传播也是这个流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层的反向传播\n",
    "1.计算某个样本的某个过滤器的dA:\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "$W_c$是表示这个过滤器。\n",
    "\n",
    "这个公式对应的python代码如下：\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "2.计算某个过滤器的dW:\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "$a_{slice}$表示输入矩阵中的被卷积的子矩阵。\n",
    "\n",
    "上面的公式对应于下面的python代码:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "3.计算某个过滤器的db:\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "上面的公式对应于下面的python代码:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"    \n",
    "    参数:\n",
    "    dZ -- 后一层相关的dZ，维度是(m, n_H, n_W, n_C)\n",
    "    cache -- 前面的conv_forward()函数保存下来的缓存数据\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- 本卷积层输入矩阵的dA，维度是(m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- 本卷积层相关的dW,维度是(f, f, n_C_prev, n_C)\n",
    "    db -- 本卷积层相关的db,维度是(1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"] # 步长\n",
    "    pad = hparameters[\"pad\"] # padding数量\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # 遍历每一个样本\n",
    "        \n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # 遍历输出矩阵的高\n",
    "            for w in range(n_W):               # 遍历输出矩阵的宽\n",
    "                for c in range(n_C):           # 遍历输出矩阵的深度\n",
    "                    \n",
    "                    # 计算输入矩阵中的子矩阵的索引\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # 取出当前进行卷积的子矩阵\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # 用上面的公式来计算偏导数\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "# print(dA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层最大池化的反向传播\n",
    "在实现池化层之前，我们需要先实现两个工具函数，第一个是 `create_mask_from_window()`，它可以根据输入矩阵得到一个特殊的输出矩阵，这个输出矩阵中只有最大值处是1，其余都是零。如下所示，X是输入矩阵，M是函数的输出矩阵: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "提示:\n",
    "- [np.max()]()会返回矩阵中的最大元素。\n",
    "- python语法`A = (X == x)`会生成一个矩阵A，这个A与X的维度是一样的，A里面其它元素都为0，只有与小x的值相同的位置处为1，也就是为True。python中0等于False，1等于True:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_windows(x):\n",
    "    \n",
    "    #x是一个矩阵。np.max(x)会得到最大的元素\n",
    "    #mask是一个与x维度相同的矩阵,可以理解为mask先是被x赋值，\n",
    "    #然后看里面的元素是不是等于max（x）的值\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "#单元测试\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_windows(x)\n",
    "print('x = ', x)\n",
    "print('mask = ', mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层平均池化的反向传播\n",
    "为了实现最大池化的反向传播，我们需要实现如下的工具函数`distribute_value`。就是把一个数值平分成一个矩阵，例如把1平分成四分之一到一个矩阵中: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    dz -- 一个数\n",
    "    shape -- 输出矩阵的维度\n",
    "    \n",
    "    返回值：\n",
    "    a -- a的维度就是shape，里面的值由dz平分而来的\n",
    "    \"\"\"\n",
    "    \n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    #计算平均值\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    #构建输出矩阵\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed_value =  [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed_value = ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dA -- 本池化层的输出矩阵对应的偏导数\n",
    "    cache -- 前向传播时缓存起来的数值\n",
    "    mode -- 是最大池化还是平均池化，(\"max\" 或 \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- 本池化层的输入矩阵对应的偏导数\n",
    "    \"\"\"\n",
    "\n",
    "    # A_prev是本池化层的输入矩阵\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                     \n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                  \n",
    "            for w in range(n_W):             \n",
    "                for c in range(n_C):          \n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        mask = create_mask_from_windows(a_prev_slice)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f, f)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
