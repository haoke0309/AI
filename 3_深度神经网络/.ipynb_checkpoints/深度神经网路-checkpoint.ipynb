{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一个人工智能程序中，使用对是单神经元神经网络，本次使用深度神经网络来提升预测精准度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数编写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    功能 ：初始化参数w和b\n",
    "    \n",
    "    参数 ：\n",
    "    layer_dims -- 每层神经元的个数\n",
    "    \n",
    "    返回值 ：\n",
    "    parameters -- 包含每层对应的初始化好的w和b\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        #构建并随机初始化该层的W，根据《1.4.3 核对矩阵的维度》\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n",
    "        \n",
    "        #构建并初始化b\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        #核对一下w和b的维度是我们预期的维度\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l],layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l],1))\n",
    "        \n",
    "    #利用上面的循环，就可以为任意层数的神经网络进行参数初始化，只需要提供给每层神经元的个数\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_forward用于实现公式 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，这个称之为线性前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    功能 ：实现上面的公式\n",
    "    \n",
    "    参数 ： \n",
    "    A -- 本层的特征输入\n",
    "    W -- 本层的相关w\n",
    "    b -- 本层的相关b\n",
    "    \n",
    "    返回值 ：\n",
    "    Z -- 根据公式计算出的本层的Z\n",
    "    cache -- 由A， W， b组成的元组\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z,cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forward(A, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.加激活后变为非线性前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_forward用于实现公式 $A^{[l]} = g(Z^{[l]})$，g代表激活函数，使用了激活函数之后上面的线性前向传播就变成了非线性前向传播了。在dnn_utils.py中我们自定义了两个激活函数，sigmoid和relu。它们都会根据传入的Z计算出A。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    功能 ：为前向传播加上激活函数\n",
    "    \n",
    "    参数 ：\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A，第一层时为特征输入\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 在工具函数中定义有 “sigmoid”和“relu” 指示该层使用的激活函数\n",
    "    \n",
    "    返回值 ：\n",
    "    A -- 激活后的A值\n",
    "    cache -- 由linear_cache 和 Z 组成的元组\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0],A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n"
     ]
    }
   ],
   "source": [
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.完整的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    功能 ：完整的前向传播过程，前L-1层使用relu，最后一层使用sigmoid\n",
    "    \n",
    "    参数 ：\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 由w和b组成的list，含有每一层的w和b\n",
    "    \n",
    "    返回值 ：\n",
    "    AL -- 即预测值y‘\n",
    "    cache -- 由A， W， b， Z 组成的元组，保存这些值在反向传播中使用\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    #获取参数列表的长度，[w1,b1,w2,b2,w3,b3,...], 所以/2得到神经网络层数\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    #for L-1次， 就是进行L-1步前向传播，每一步使用激活函数relu\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                            parameters['W' + str(l)],\n",
    "                                            parameters['b' + str(1)],\n",
    "                                            activation = 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    #进行最后一层的前向传播，激活函数是sigmoid，得出的AL就是预测值y‘\n",
    "    AL, cache = linear_activation_forward(A,\n",
    "                                         parameters['W' + str(L)],\n",
    "                                         parameters['b' + str(L)],\n",
    "                                         activation = 'sigmoid')\n",
    "    \n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, cache\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.计算成本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    功能 ： 计算成本\n",
    "    \n",
    "    参数 ：\n",
    "    AL -- 即预测值y‘\n",
    "    Y -- 特征数据对应的标签数据\n",
    "    \n",
    "    返回值：\n",
    "    cost -- 成本\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1 - Y),np.log(1 - AL)))\n",
    "    \n",
    "    #确保cost是一个值，而不是一个数组的形式\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_backward函数用于根据后一层的dZ来计算前面一层的dW，db和dA。也就是实现了下面3个公式\n",
    "$$ dW^{[l]}  = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]}  = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    功能 ： 实现上面的三个公式，根据后一层的dZ来计算前面一层的dW，db，dA\n",
    "    \n",
    "    参数 ：\n",
    "    dZ -- 后面一层的dz\n",
    "    cache -- 前向传播保存的 由A， W， b， Z 组成的元组\n",
    "    \n",
    "    返回值 ：\n",
    "    dA_prev -- 前一层的dA\n",
    "    dW -- 前一层的dW\n",
    "    db -- 前一层的db\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单元测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.加激活后的反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的linear_activation_backward用于根据本层的dA计算出本层的dZ。就是实现了下面的公式\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "上式的g'()表示求Z相当于本层的激活函数的偏导数。所以不同的激活函数也有不同的求导公式。\n",
    "我们为大家编写了两个求导函数sigmoid_backward和relu_backward。大家当前不需要关心这两个函数的内部实现，当然，如果你感兴趣可以到dnn_utils.py里面去看它们的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    功能 ：加上激活函数的反向传播\n",
    "    \n",
    "    参数 ： \n",
    "    dA -- \n",
    "    cache --\n",
    "    activation --\n",
    "    \n",
    "    返回值 ：\n",
    "    dA_prev --\n",
    "    dW --\n",
    "    db --\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
